# SISTEMA DE RECONOCIMIENTO DE EMOCIONES MULTIMODAL CON ARQUITECTURA DE MICROSERVICIOS
## An√°lisis de Emociones en Im√°genes Faciales y Texto mediante Deep Learning y NLP

**Autor:** Omar Condori Pachauri  
**Instituci√≥n:** [Tu Universidad/Instituci√≥n]  
**Fecha:** Noviembre 2025

---

## RESUMEN EJECUTIVO

El presente proyecto detalla el dise√±o, desarrollo e implementaci√≥n de un sistema de reconocimiento de emociones multimodal capaz de analizar tanto expresiones faciales en im√°genes como sentimientos en texto escrito. La soluci√≥n integra tecnolog√≠as de vanguardia en Inteligencia Artificial, utilizando Redes Neuronales Convolucionales (CNN) para el procesamiento de im√°genes y modelos basados en Transformers (DistilBERT) para el Procesamiento de Lenguaje Natural (NLP).

La arquitectura del sistema se basa en un enfoque de microservicios, desacoplando la l√≥gica de negocio (gestionada por un backend en Java con Spring Boot) de los servicios de inferencia de IA (gestionados por un backend en Python con FastAPI). Esta estructura garantiza escalabilidad, mantenibilidad y un rendimiento √≥ptimo. El sistema se completa con una aplicaci√≥n m√≥vil desarrollada en Flutter, proporcionando una interfaz de usuario intuitiva y accesible. Los resultados experimentales demuestran la eficacia de las arquitecturas seleccionadas frente a m√©todos tradicionales, validando la viabilidad t√©cnica de la soluci√≥n propuesta.

---

## 1. INTRODUCCI√ìN

### 1.1 Contexto y Motivaci√≥n
En la era digital actual, la interacci√≥n humano-computadora ha evolucionado m√°s all√° de los comandos simples. La capacidad de las m√°quinas para identificar y responder a las emociones humanas es un campo de investigaci√≥n crucial conocido como Computaci√≥n Afectiva. Las aplicaciones son vastas, abarcando desde el monitoreo de la salud mental y la detecci√≥n temprana de depresi√≥n, hasta la mejora de la atenci√≥n al cliente mediante el an√°lisis de sentimientos en tiempo real y la personalizaci√≥n de experiencias educativas.

Sin embargo, la mayor√≠a de los sistemas actuales se limitan a una sola modalidad (solo texto o solo imagen). La comunicaci√≥n humana es intr√≠nsecamente multimodal; una frase ir√≥nica puede tener un significado opuesto dependiendo de la expresi√≥n facial que la acompa√±e. Por ello, existe una necesidad creciente de sistemas que integren m√∫ltiples fuentes de datos para una comprensi√≥n m√°s hol√≠stica y precisa del estado emocional del usuario.

### 1.2 Objetivos del Proyecto
**Objetivo General:**
Desarrollar un sistema integral de reconocimiento de emociones multimodal que combine el an√°lisis de expresiones faciales y texto mediante arquitecturas de Deep Learning, desplegado sobre una infraestructura de microservicios escalable.

**Objetivos Espec√≠ficos:**
1.  Implementar y entrenar una Red Neuronal Convolucional (CNN) optimizada para la clasificaci√≥n de 7 emociones b√°sicas en im√°genes faciales.
2.  Implementar y evaluar modelos de NLP (Bi-LSTM, CNN 1D y Transformers) para la detecci√≥n de emociones en texto en espa√±ol.
3.  Dise√±ar una arquitectura de microservicios que integre un backend robusto en Java (Spring Boot) con un servicio de inferencia en Python (FastAPI).
4.  Desarrollar una aplicaci√≥n m√≥vil multiplataforma (Flutter) que sirva como interfaz de usuario para la captura y visualizaci√≥n de resultados.
5.  Evaluar el rendimiento de las diferentes arquitecturas de IA implementadas mediante m√©tricas de precisi√≥n y p√©rdida.

### 1.3 Alcance
El proyecto abarca desde la recolecci√≥n y preprocesamiento de datos (datasets FER-2013 y EmoEvent) hasta el despliegue de los servicios y la aplicaci√≥n m√≥vil. Incluye el entrenamiento de modelos de IA, la creaci√≥n de APIs RESTful, la gesti√≥n de base de datos PostgreSQL y la integraci√≥n de herramientas de tunelizaci√≥n (ngrok) para pruebas remotas.
**Limitaciones:** El sistema requiere conexi√≥n a internet para procesar las solicitudes en el servidor. El an√°lisis de video en tiempo real no est√° incluido en esta fase, limit√°ndose a captura de im√°genes est√°ticas.

### 1.4 Estructura del Documento
El documento se organiza en cap√≠tulos que detallan cada aspecto del desarrollo. El **Marco Te√≥rico** establece las bases conceptuales de las emociones y las redes neuronales. La **Metodolog√≠a** describe el dise√±o de la arquitectura y los algoritmos. Los **Resultados** presentan las m√©tricas de evaluaci√≥n de los modelos, y finalmente, las **Conclusiones** resumen los hallazgos y proponen trabajos futuros.

---

## 2. MARCO TE√ìRICO

### 2.1 Reconocimiento de Emociones
El reconocimiento de emociones se fundamenta en la teor√≠a de las emociones b√°sicas de Paul Ekman, quien identific√≥ seis emociones universales (ira, asco, miedo, alegr√≠a, tristeza y sorpresa) que se manifiestan a trav√©s de expresiones faciales consistentes entre culturas. El reconocimiento autom√°tico busca replicar esta capacidad humana utilizando algoritmos que analizan patrones en datos visuales, auditivos o textuales.

### 2.2 Arquitecturas de Redes Neuronales para An√°lisis de Emociones

#### 2.2.1 Para Procesamiento de Im√°genes:

**Redes Neuronales Convolucionales (CNN)**
Las CNN son un tipo de red neuronal profunda dise√±ada espec√≠ficamente para procesar datos con estructura de rejilla, como las im√°genes. A diferencia de las redes tradicionales que aplanan la entrada, las CNN conservan la estructura espacial 2D.
*   **Funcionamiento:** Utilizan capas de convoluci√≥n que aplican filtros (kernels) a la imagen para extraer caracter√≠sticas (bordes, texturas, formas). Estas caracter√≠sticas se reducen mediante capas de *Pooling* y finalmente se clasifican mediante capas *Fully Connected*.
*   **Justificaci√≥n de uso:** En este proyecto utilizamos CNNs porque son el estado del arte en visi√≥n computacional. Su capacidad para capturar patrones espaciales locales y su invarianza a la traslaci√≥n las hacen ideales para detectar microexpresiones faciales independientemente de la posici√≥n del rostro en la imagen. Adem√°s, reducen dr√°sticamente el n√∫mero de par√°metros comparado con un Perceptr√≥n Multicapa (MLP) convencional.

#### 2.2.2 Para Procesamiento de Texto (NLP):

**Multi-Layer Perceptron (MLP)**
El Perceptr√≥n Multicapa (MLP) es la arquitectura m√°s b√°sica de red neuronal profunda. Consiste en una capa de entrada, una o m√°s capas ocultas y una capa de salida. Cada neurona est√° conectada a todas las neuronas de la capa siguiente (Fully Connected).
*   **Limitaciones en NLP:** El MLP trata cada palabra como una caracter√≠stica independiente (Bag of Words) o requiere una entrada de tama√±o fijo, perdiendo la informaci√≥n secuencial y el orden de las palabras. Por ejemplo, no distingue f√°cilmente entre "El perro mordi√≥ al hombre" y "El hombre mordi√≥ al perro".
*   **Uso en el proyecto:** Se descart√≥ como modelo principal debido a su incapacidad para capturar contexto sem√°ntico complejo.

**Long Short-Term Memory (LSTM) Unidireccional**
Las LSTM son una variante avanzada de las Redes Neuronales Recurrentes (RNN). Las RNN tradicionales sufren del problema del "desvanecimiento del gradiente", lo que les impide aprender dependencias a largo plazo.
*   **Arquitectura:** Una celda LSTM introduce tres "puertas" (gates):
    1.  **Forget Gate:** Decide qu√© informaci√≥n descartar del estado de la celda.
    2.  **Input Gate:** Decide qu√© nueva informaci√≥n almacenar.
    3.  **Output Gate:** Decide qu√© parte del estado de la celda enviar a la salida.
*   **Ventaja:** Permite recordar informaci√≥n relevante (como el g√©nero de un sujeto) a lo largo de muchas palabras para realizar concordancias gramaticales o sem√°nticas al final de la oraci√≥n.

**Long Short-Term Memory (LSTM) Bidireccional (Bi-LSTM)**
Una limitaci√≥n de la LSTM unidireccional es que solo ve el "pasado" (palabras anteriores). La Bi-LSTM entrena dos LSTMs separadas: una procesa la secuencia de izquierda a derecha y la otra de derecha a izquierda.
*   **Funcionamiento:** Los resultados de ambas direcciones se concatenan en cada paso de tiempo.
*   **Ventaja:** Permite que el modelo entienda el contexto completo de una palabra bas√°ndose tanto en lo que se dijo antes como en lo que se dir√° despu√©s. Es ideal para tareas de clasificaci√≥n de texto donde toda la oraci√≥n est√° disponible.
*   **Implementaci√≥n y Evoluci√≥n:** Fue nuestro primer modelo de red neuronal implementado por su buen balance entre precisi√≥n y costo computacional. *Nota: Esta arquitectura sirvi√≥ como prototipo inicial y l√≠nea base para comparar el rendimiento con modelos m√°s avanzados.*

**Transformers**
Introducidos en 2017 por Google ("Attention is All You Need"), los Transformers abandonan la recurrencia (procesamiento secuencial) en favor del mecanismo de **Atenci√≥n (Self-Attention)**.
*   **Mecanismo de Atenci√≥n:** Permite que el modelo asigne un peso de importancia a cada palabra de la frase en relaci√≥n con la palabra que est√° procesando actualmente. Esto captura relaciones sem√°nticas complejas independientemente de la distancia entre palabras.
*   **Paralelizaci√≥n:** Al no ser secuenciales, los Transformers pueden procesar toda la frase de golpe, aprovechando masivamente las GPUs.
*   **BERT / DistilBERT:** Utilizamos DistilBERT, una versi√≥n m√°s ligera y r√°pida de BERT (Bidirectional Encoder Representations from Transformers). BERT se pre-entrena con millones de textos para entender el lenguaje humano y luego se hace "fine-tuning" con nuestros datos espec√≠ficos de emociones.
*   **Justificaci√≥n:** Es el estado del arte actual. Ofrece la m√°xima precisi√≥n disponible para tareas de NLP.

**Tabla Comparativa de Arquitecturas NLP:**

| Arquitectura | Ventajas | Desventajas | Precisi√≥n T√≠pica | Complejidad |
|--------------|----------|-------------|------------------|-------------|
| MLP | Simple, r√°pido | No captura secuencias ni orden | Baja | Baja |
| LSTM Uni | Memoria temporal | Solo contexto pasado | Media | Media |
| LSTM Bi | Contexto completo | M√°s lento que Uni | Alta | Media-Alta |
| Transformer | Estado del arte, atenci√≥n global | Muy costoso computacionalmente | Muy Alta | Muy Alta |

### 2.3 Arquitectura de Microservicios
La arquitectura de microservicios estructura una aplicaci√≥n como una colecci√≥n de servicios peque√±os, aut√≥nomos y d√©bilmente acoplados. Cada servicio se ejecuta en su propio proceso y se comunica mediante mecanismos ligeros (generalmente HTTP/REST).

#### Justificaci√≥n T√©cnica de la Separaci√≥n de Backends

1.  **Separaci√≥n de Preocupaciones (Separation of Concerns)**
    *   **Python**: Se especializa en IA/ML y procesamiento de modelos.
    *   **Java**: Se especializa en l√≥gica de negocio, validaciones y persistencia.

2.  **Optimizaci√≥n por Fortalezas**
    *   Python es m√°s r√°pido para el desarrollo e inferencia de redes neuronales gracias a librer√≠as como TensorFlow y PyTorch.
    *   Java es m√°s eficiente para operaciones CRUD, transacciones bancarias y gesti√≥n de concurrencia a nivel empresarial.

3.  **Escalabilidad Independiente**
    *   El servicio de Python puede escalar horizontalmente (m√°s r√©plicas) si la carga de inferencias aumenta, sin necesidad de duplicar el servicio de Java.
    *   El servicio de Java puede escalar si aumenta el tr√°fico de la API (usuarios consultando historial), sin duplicar los modelos de IA pesados.

4.  **Mantenibilidad y Resiliencia**
    *   Equipos especializados pueden trabajar independientemente en cada microservicio.
    *   Si el servicio de IA falla o se reinicia, el servicio Java puede manejar el error y responder al usuario sin que toda la aplicaci√≥n colapse.

**Tabla Comparativa: Python vs Java**

| Aspecto | Python (FastAPI) | Java (Spring Boot) |
|---------|------------------|-------------------|
| IA/ML | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excelente | ‚≠ê‚≠ê Limitado |
| Performance IA | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| L√≥gica de Negocio | ‚≠ê‚≠ê‚≠ê Bueno | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excelente |
| ORM/Persistencia | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Tipado | ‚≠ê‚≠ê‚≠ê Din√°mico | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Est√°tico |
| Ecosistema IA | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| Escalabilidad | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Curva de Aprendizaje | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê F√°cil | ‚≠ê‚≠ê‚≠ê Media |

**Conclusi√≥n**: La combinaci√≥n de ambos backends aprovecha las fortalezas de cada tecnolog√≠a, resultando en un sistema m√°s robusto y eficiente.

### 2.4 Tecnolog√≠as Utilizadas

#### 2.4.1 Backend Python con FastAPI
**¬øPor qu√© Python?**
Python es el est√°ndar de facto en Inteligencia Artificial. Su ecosistema (TensorFlow, PyTorch, NumPy, Scikit-learn) es inigualable, permitiendo la implementaci√≥n directa de modelos complejos.

**¬øPor qu√© FastAPI?**
FastAPI es un framework moderno de alto rendimiento. A diferencia de Flask, es as√≠ncrono nativo (`async/await`), lo que es crucial para manejar m√∫ltiples solicitudes de inferencia sin bloquear el servidor.

#### 2.4.2 Backend Java con Spring Boot
**¬øPor qu√© Java?**
Java aporta robustez, tipado est√°tico y un rendimiento excepcional para la l√≥gica de negocio y la gesti√≥n de transacciones. Es ideal para construir sistemas empresariales seguros y mantenibles.

**¬øPor qu√© Spring Boot?**
Spring Boot simplifica la creaci√≥n de microservicios listos para producci√≥n. Su ecosistema (Spring Data, Spring Security) facilita la integraci√≥n con bases de datos y la gesti√≥n de la seguridad.

### 2.5 Datasets Utilizados

#### 2.5.1 FER-2013 (Facial Emotion Recognition)
*   **Origen**: Desaf√≠o ICML 2013.
*   **Composici√≥n**: 35,887 im√°genes en escala de grises de 48x48 p√≠xeles.
*   **Clases**: 7 emociones (Ira, Asco, Miedo, Alegr√≠a, Tristeza, Sorpresa, Neutral).
*   **Desaf√≠os**: Im√°genes "in-the-wild" (no posadas), baja resoluci√≥n, desbalanceo de clases (muchas de 'Alegr√≠a', pocas de 'Asco').
*   **Link**: [Kaggle FER-2013](https://www.kaggle.com/datasets/msambare/fer2013)

#### 2.5.2 EmoEvent Corpus
*   **Origen**: Acad√©mico, enfocado en tweets en espa√±ol.
*   **Composici√≥n**: ~8,400 tweets etiquetados.
*   **Clases**: 7 emociones (Alegr√≠a, Tristeza, Ira, Miedo, Sorpresa, Asco, Otros).
*   **Desaf√≠os**: Uso de lenguaje informal, sarcasmo, modismos y contexto cultural espec√≠fico.
*   **Link**: [GitHub EmoEvent](https://github.com/fmplaza/EmoEvent)

---

## 3. METODOLOG√çA

### 3.1 Arquitectura General del Sistema

**Diagrama de Arquitectura Completo:**

```mermaid
graph LR
    Mobile[üì± App Flutter] -->|HTTP POST| Java[‚òï Backend Java\n(Spring Boot)]
    Java <-->|JDBC| DB[(üóÑÔ∏è PostgreSQL)]
    Java <-->|HTTP JSON| Python[üêç Backend Python\n(FastAPI + IA)]
    
    subgraph "Servidor de Inteligencia Artificial"
        Python --> Model1[üñºÔ∏è Modelo CNN\n(Im√°genes)]
        Python --> Model2[üìù Modelo Transformer\n(Texto)]
    end
```

### 3.2 M√≥dulo de Reconocimiento Facial

#### 3.2.1 Preprocesamiento
Normalizaci√≥n de p√≠xeles (0-1) y Data Augmentation (rotaci√≥n, zoom) para mejorar la generalizaci√≥n.

#### 3.2.2 Arquitectura CNN
Se dise√±√≥ una CNN con 4 bloques convolucionales, cada uno seguido de BatchNormalization, MaxPooling y Dropout para evitar overfitting.
*   **Archivo de Entrenamiento**: `python-service/train_model.py`
*   **Par√°metros**: 7,187,911
*   **Hardware**: Apple M3 Pro (GPU Metal)

### 3.3 M√≥dulo de An√°lisis de Texto

#### 3.3.1 Preprocesamiento
Tokenizaci√≥n utilizando el tokenizador de DistilBERT, padding a 128 tokens y creaci√≥n de m√°scaras de atenci√≥n.

#### 3.3.2 Arquitectura NLP
Se implementaron y compararon m√∫ltiples arquitecturas:
1.  **Bi-LSTM**: Red recurrente bidireccional. *Nota: Implementada inicialmente como prototipo.*
2.  **CNN 1D**: Red convolucional para texto.
3.  **Transformer (DistilBERT)**: Modelo pre-entrenado fine-tuneado. *Nota: Modelo final seleccionado para producci√≥n.*
*   **Archivo de Entrenamiento**: `python-service/train_nlp.py` (Versi√≥n actual con Transformer)
*   **Archivo de Experimentos**: `python-service/experiments.py` (Contiene CNN 1D, SVM, Naive Bayes)

### 3.4 Integraci√≥n de Microservicios
El flujo de datos comienza en la App m√≥vil, pasa al backend Java para validaci√≥n y registro, y finalmente llega al backend Python para la inferencia. La respuesta sigue el camino inverso.

---

## 4. RESULTADOS

### 4.1 M√©tricas de Rendimiento

#### 4.1.1 Reconocimiento Facial (CNN)
*   **Precisi√≥n Global**: 64.18%
*   **An√°lisis**: El modelo muestra un excelente desempe√±o en 'Alegr√≠a' y 'Sorpresa', con mayor dificultad en diferenciar 'Miedo' de 'Sorpresa' debido a similitudes visuales.

#### 4.1.2 An√°lisis de Texto (Comparativa)

Se realizaron experimentos con 5 arquitecturas diferentes utilizando el dataset EmoEvent.

**1. Machine Learning Tradicional**
*   **Naive Bayes (MultinomialNB)**:
    *   Precisi√≥n: **53.03%**
    *   Comentario: Modelo base, r√°pido pero limitado en contexto.
*   **SVM (Support Vector Machine)**:
    *   Precisi√≥n: **58.00%**
    *   Comentario: Excelente rendimiento para ser un modelo tradicional, muy competitivo.

**2. Redes Neuronales**
*   **Bi-LSTM**:
    *   Precisi√≥n: **~48%**
    *   Comentario: Buen manejo de secuencias, pero superado por SVM en este dataset espec√≠fico. *Este modelo sirvi√≥ como l√≠nea base inicial.*
*   **CNN 1D**:
    *   Precisi√≥n: **55.77%**
    *   Comentario: R√°pida y efectiva para capturar patrones locales en texto.
*   **Transformer (DistilBERT)**:
    *   Precisi√≥n: **~58.15%**
    *   Comentario: El modelo m√°s robusto sem√°nticamente. Aunque su precisi√≥n num√©rica es similar a SVM, su capacidad de generalizaci√≥n ante frases complejas es superior.

### 4.2 Rendimiento del Sistema
*   **Tiempo de Inferencia Imagen**: ~150ms
*   **Tiempo de Inferencia Texto**: ~100ms
*   **Latencia Total (App -> Respuesta)**: ~250ms

---

## 5. CONCLUSIONES

### 5.1 Logros Alcanzados
‚úÖ Sistema completo funcional con arquitectura de microservicios.
‚úÖ Modelo CNN optimizado para hardware Apple Silicon.
‚úÖ Implementaci√≥n exitosa de Transformers para NLP.
‚úÖ Aplicaci√≥n m√≥vil intuitiva y responsiva.

### 5.2 Ventajas de la Arquitectura
La decisi√≥n de separar los backends en Java y Python demostr√≥ ser acertada. Permiti√≥ aprovechar las librer√≠as de IA de Python sin sacrificar la robustez empresarial de Java. La comunicaci√≥n v√≠a REST es fluida y la latencia es imperceptible para el usuario final.

### 5.3 Trabajo Futuro
*   Implementar detecci√≥n de emociones en tiempo real (video).
*   Explorar modelos de lenguaje m√°s grandes (BERT-Large, RoBERTa).
*   A√±adir autenticaci√≥n de usuarios con JWT.

---

## 6. REFERENCIAS BIBLIOGR√ÅFICAS

1.  Goodfellow, I., et al. (2013). "Challenges in Representation Learning: A report on three machine learning contests".
2.  Vaswani, A., et al. (2017). "Attention Is All You Need".
3.  **Dataset FER-2013**: https://www.kaggle.com/datasets/msambare/fer2013
4.  **Dataset EmoEvent**: https://github.com/fmplaza/EmoEvent
5.  **TensorFlow Documentation**: https://www.tensorflow.org/
6.  **Spring Boot Documentation**: https://spring.io/projects/spring-boot
